\section{Токенизация}

\subsection{Определение и цели}

\textbf{Токенизация} --- процесс разбиения текста на элементарные единицы (токены), которые используются для построения поискового индекса. Токен --- последовательность символов, представляющая слово или другую значимую лексическую единицу.

Цели токенизации:
\begin{itemize}
    \item Выделение значимых единиц текста из потока символов
    \item Нормализация: приведение к единообразному виду (lowercase)
    \item Фильтрация шума: удаление HTML-артефактов, служебных символов
    \item Формирование словаря для индексации
\end{itemize}

\subsection{Реализация}

Токенизатор реализован в виде класса \texttt{Tokenizer} на C++. Основная функция \texttt{tokenize} принимает строку текста и возвращает вектор токенов. Каждый токен содержит текст и позицию в исходном документе.

Алгоритм работы (однопроходный, сложность $O(n)$):
\begin{enumerate}
    \item Последовательное чтение символов из входной строки
    \item Определение типа символа: кириллический (UTF-8, двухбайтовый), латинский, цифра или разделитель
    \item Накопление символов в буфере до встречи разделителя
    \item При встрече разделителя --- проверка валидности токена
    \item Если токен валиден --- добавление в результирующий список
\end{enumerate}

\subsection{Правила токенизации}

\subsubsection{Разделители}

\begin{itemize}
    \item Пробельные символы: пробел, табуляция, перевод строки
    \item Знаки препинания: точка, запятая, восклицательный и вопросительный знаки, точка с запятой, двоеточие
    \item Скобки: круглые, квадратные, фигурные
    \item Кавычки: одинарные и двойные
    \item Специальные символы: угловые скобки, слеши, амперсанды
\end{itemize}

\subsubsection{Обработка UTF-8 кириллицы}

Кириллические символы в UTF-8 представлены двухбайтовыми последовательностями. Токенизатор проверяет первый байт (0xD0 или 0xD1) и читает следующий байт для формирования полного символа. Приведение к нижнему регистру выполняется на уровне байтов UTF-8.

\subsubsection{Фильтрация токенов}

\begin{enumerate}
    \item Токены короче 2 символов отбрасываются
    \item Последовательности, состоящие только из цифр, удаляются
    \item Все символы приводятся к нижнему регистру
\end{enumerate}

\subsection{Достоинства метода}

\begin{enumerate}
    \item \textbf{Производительность}: однопроходный алгоритм, $O(n)$ по длине текста
    \item \textbf{Корректная работа с UTF-8}: побайтовый анализ кириллицы без использования сторонних библиотек
    \item \textbf{Низкое потребление памяти}: потоковая обработка без загрузки всего корпуса
    \item \textbf{Детерминированность}: одинаковый результат при повторной обработке
    \item \textbf{Универсальность}: работает с русским и английским языками
\end{enumerate}

\subsection{Недостатки метода}

\begin{enumerate}
    \item \textbf{Отсутствие контекстного анализа}: слова-омонимы не различаются
    \item \textbf{Проблемы с составными словами}: <<военно-морской>> разбивается на <<военно>> и <<морской>>
    \item \textbf{Аббревиатуры}: <<т.е.>>, <<и т.д.>> разбиваются по точкам
    \item \textbf{Числовые конструкции}: <<1945 год>> --- число отфильтровывается
    \item \textbf{Имена собственные}: <<Лев Толстой>> обрабатывается как два отдельных токена
\end{enumerate}

\subsection{Журнал выполнения задания}

При реализации токенизатора были выявлены следующие проблемы и их решения:

\begin{enumerate}
    \item \textbf{Обработка UTF-8 кириллицы}: Основная сложность заключалась в корректной обработке двухбайтовых последовательностей UTF-8 для кириллических символов. Стандартные функции C++ (\texttt{std::tolower}) не работают напрямую с UTF-8. Решение: побайтовый анализ с проверкой первого байта (0xD0 или 0xD1) и чтением второго байта для формирования полного символа. Приведение к нижнему регистру выполнено через таблицу соответствий для кириллицы.
    
    \item \textbf{Обработка дефисов в составных словах}: Слова типа <<военно-морской>>, <<научно-фантастический>> разбивались на отдельные токены из-за того, что дефис рассматривался как разделитель. Это приводило к потере семантической целостности терминов. Решение: дефис оставлен как разделитель, так как альтернативный подход (сохранение дефисов как части токена) создавал бы проблемы с поиском отдельных компонентов составных слов.
    
    \item \textbf{Смешанный текст (русский и латинский)}: В корпусе встречаются лингвистические термины и названия на латинице (например, <<phoneme>>, <<IPA>>). Токенизатор должен корректно обрабатывать оба алфавита. Решение: раздельная обработка кириллических и латинских символов с едиными правилами фильтрации и нормализации.
    
    \item \textbf{Производительность на больших объёмах}: При обработке корпуса из 35\,000 документов (~50 миллионов токенов) важна эффективность алгоритма. Решение: однопроходный алгоритм без создания промежуточных структур данных, использование перемещения (\texttt{std::move}) для векторов токенов.
\end{enumerate}

\subsection{Примеры проблемных случаев}

\begin{table}[H]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Исходный текст} & \textbf{Результат} & \textbf{Проблема} \\
\midrule
<<Лев Толстой>> & [<<лев>>, <<толстой>>] & Разбиение имени \\
<<т.е.>> & [<<те>>] & Точки удалены \\
<<1869 год>> & [<<год>>] & Число отфильтровано \\
<<научно-фантастический>> & [<<научно>>, <<фантастический>>] & Дефис как разделитель \\
<<XIX>> & [] & Римские цифры удалены \\
<<non-fiction>> & [<<non>>, <<fiction>>] & Дефис разбил термин \\
\bottomrule
\end{tabular}
\caption{Проблемные случаи токенизации}
\end{table}

\subsection{Выводы}

Токенизатор реализован как однопроходный алгоритм с корректной обработкой UTF-8 кириллицы и обеспечивает базовую нормализацию текста для построения поискового индекса.

\textbf{Критический анализ качества токенизатора:}
\begin{itemize}
    \item \textbf{Сильные стороны}: Высокая производительность ($O(n)$), корректная работа с кириллицей без внешних библиотек, детерминированность результатов, низкое потребление памяти за счёт потоковой обработки.
    
    \item \textbf{Слабые стороны}: Отсутствие контекстного анализа приводит к проблемам с омонимами и составными словами. Фильтрация коротких токенов может удалять важные аббревиатуры и имена (например, <<А.С. Пушкин>> теряет инициалы).
    
    \item \textbf{Влияние на качество поиска}: Простые запросы (одиночные слова) обрабатываются корректно. Сложные запросы с именами собственными или составными терминами могут давать неполные результаты из-за разбиения на отдельные токены.
\end{itemize}

\textbf{Влияние на поисковую систему:}
\begin{itemize}
    \item Токенизатор формирует основу для инвертированного индекса --- каждое слово становится ключом для поиска
    \item Потеря информации при разбиении составных слов снижает точность поиска по специализированным терминам
    \item Фильтрация чисел исключает возможность поиска по датам и числовым данным (например, <<1869 год>> не найдёт документы с упоминанием этой даты)
\end{itemize}

\textbf{Возможные улучшения:}
\begin{itemize}
    \item Использование морфологического анализатора для более точного выделения границ слов
    \item Словарь исключений для составных терминов (<<военно-морской>>, <<XIX век>>)
    \item Сохранение контекста (позиция в документе) для фразового поиска
    \item Обработка аббревиатур и сокращений через специальные правила или словари
\end{itemize}

\pagebreak
