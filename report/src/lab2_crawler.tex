\section{Поисковый робот}

\subsection{Архитектура краулера}

Поисковый робот реализован на языке Python с использованием Wikipedia API для обнаружения статей и асинхронной загрузки контента. Основные библиотеки: \texttt{aiohttp} (асинхронные HTTP-запросы), \texttt{BeautifulSoup} (парсинг HTML).

Краулер работает в два этапа: сначала через API рекурсивно обходит дерево категорий и собирает названия статей, затем параллельно загружает их содержимое. Результат сохраняется в формате NDJSON.

\subsection{Алгоритм работы}

\textbf{Фаза 1 --- обнаружение статей:}
\begin{enumerate}
    \item Загрузка списка seed-категорий из конфигурационного файла \texttt{config.yaml}.
    \item Для каждой категории --- запрос к Wikipedia API (\texttt{action=query\&list=categorymembers}) для получения статей и подкатегорий.
    \item Рекурсивный обход подкатегорий (до глубины 10) с параллельным выполнением (батчи по 10).
    \item Дедупликация по заголовку статьи.
\end{enumerate}

\textbf{Фаза 2 --- загрузка контента:}
\begin{enumerate}
    \item Для каждой обнаруженной статьи --- запрос к API (\texttt{action=parse}) для получения HTML.
    \item Извлечение текста из HTML: параграфы, списки, заголовки.
    \item Фильтрация по минимальной длине текста.
    \item Дедупликация по каноническому заголовку (разрешение редиректов).
    \item Сохранение в NDJSON.
    \item Параллельная обработка (до 30 одновременных запросов).
\end{enumerate}

\subsection{Критерии фильтрации}

Для обеспечения качества корпуса применяются следующие фильтры:

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Критерий} & \textbf{Значение} \\
\midrule
Минимальная длина текста & 1\,500 символов \\
Минимальное число параграфов & 3 \\
Исключение служебных страниц & Категории, шаблоны, обсуждения, файлы \\
Исключение дубликатов & По нормализованному URL \\
\bottomrule
\end{tabular}
\caption{Критерии фильтрации документов}
\end{table}

Список исключаемых паттернов URL:
\begin{itemize}
    \item \texttt{Special:}, \texttt{Служебная:} --- служебные страницы
    \item \texttt{File:}, \texttt{Файл:} --- страницы файлов
    \item \texttt{Category:}, \texttt{Категория:} --- страницы категорий (используются только для навигации)
    \item \texttt{Template:}, \texttt{Шаблон:} --- шаблоны
    \item \texttt{Talk:}, \texttt{Обсуждение:} --- страницы обсуждений
    \item \texttt{User:}, \texttt{Участник:} --- страницы участников
    \item \texttt{Wikipedia:}, \texttt{Википедия:} --- внутренние страницы Википедии
\end{itemize}

\subsection{Формат выходных данных}

Каждая строка файла \texttt{corpus.ndjson} содержит один JSON-объект:

\begin{verbatim}
{"url": "https://ru.wikipedia.org/wiki/Роман_(литература)",
 "title": "Роман (литература)",
 "text": "Роман — литературный жанр, как правило...",
 "word_count": 4521,
 "paragraph_count": 38}
\end{verbatim}

\subsection{Обработка проблемных ситуаций}

\begin{itemize}
    \item \textbf{Таймауты}: при отсутствии ответа в течение 30 секунд запрос пропускается.
    \item \textbf{Ошибки HTTP}: страницы с кодами 4xx/5xx пропускаются.
    \item \textbf{Кодировка}: все тексты приводятся к UTF-8.
    \item \textbf{Ограничение нагрузки}: задержка между запросами предотвращает блокировку.
    \item \textbf{User-Agent}: используется идентификатор \texttt{WikiSearchBot/1.0}.
\end{itemize}

\subsection{Контейнеризация}

Краулер упакован в Docker-контейнер и запускается командой:
\begin{verbatim}
docker-compose up scraper
\end{verbatim}

После завершения работы корпус доступен в директории \texttt{data/corpus.ndjson}.

\pagebreak
