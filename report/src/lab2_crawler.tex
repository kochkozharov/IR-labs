\section{Поисковый робот}

\subsection{Архитектура краулера}

Поисковый робот реализован на языке Python с использованием асинхронного подхода для параллельной загрузки страниц. Основные библиотеки: \texttt{aiohttp} (асинхронные HTTP-запросы), \texttt{BeautifulSoup} (парсинг HTML).

Краулер запускается однократно, обходит заданные категории Википедии и сохраняет собранный корпус в файл формата NDJSON (Newline Delimited JSON).

\subsection{Алгоритм работы}

\begin{enumerate}
    \item Инициализация очереди начальными URL категорий из конфигурационного файла \texttt{config.yaml}.
    \item Параллельная загрузка страниц (до 10 одновременных запросов) с ограничением частоты (100 мс между запросами).
    \item Извлечение текста из HTML: удаление служебной разметки Википедии, навигационных блоков, таблиц, инфобоксов.
    \item Извлечение внутренних ссылок для продолжения обхода.
    \item Фильтрация по критериям качества.
    \item Сохранение прошедших фильтрацию документов в NDJSON.
    \item Добавление новых ссылок в очередь (до глубины 4).
    \item Остановка при достижении лимита документов (35\,000).
\end{enumerate}

\subsection{Критерии фильтрации}

Для обеспечения качества корпуса применяются следующие фильтры:

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Критерий} & \textbf{Значение} \\
\midrule
Минимальная длина текста & 1\,500 символов \\
Минимальное число параграфов & 3 \\
Исключение служебных страниц & Категории, шаблоны, обсуждения, файлы \\
Исключение дубликатов & По нормализованному URL \\
\bottomrule
\end{tabular}
\caption{Критерии фильтрации документов}
\end{table}

Список исключаемых паттернов URL:
\begin{itemize}
    \item \texttt{Special:}, \texttt{Служебная:} --- служебные страницы
    \item \texttt{File:}, \texttt{Файл:} --- страницы файлов
    \item \texttt{Category:}, \texttt{Категория:} --- страницы категорий (используются только для навигации)
    \item \texttt{Template:}, \texttt{Шаблон:} --- шаблоны
    \item \texttt{Talk:}, \texttt{Обсуждение:} --- страницы обсуждений
    \item \texttt{User:}, \texttt{Участник:} --- страницы участников
    \item \texttt{Wikipedia:}, \texttt{Википедия:} --- внутренние страницы Википедии
\end{itemize}

\subsection{Формат выходных данных}

Каждая строка файла \texttt{corpus.ndjson} содержит один JSON-объект:

\begin{verbatim}
{"url": "https://ru.wikipedia.org/wiki/Лингвистика",
 "title": "Лингвистика",
 "text": "Лингвистика — наука, изучающая языки...",
 "word_count": 5234,
 "paragraph_count": 47}
\end{verbatim}

\subsection{Обработка проблемных ситуаций}

\begin{itemize}
    \item \textbf{Таймауты}: при отсутствии ответа в течение 30 секунд запрос пропускается.
    \item \textbf{Ошибки HTTP}: страницы с кодами 4xx/5xx пропускаются.
    \item \textbf{Кодировка}: все тексты приводятся к UTF-8.
    \item \textbf{Ограничение нагрузки}: задержка между запросами предотвращает блокировку.
    \item \textbf{User-Agent}: используется идентификатор \texttt{LinguisticsSearchBot/1.0}.
\end{itemize}

\subsection{Контейнеризация}

Краулер упакован в Docker-контейнер и запускается командой:
\begin{verbatim}
docker-compose up scraper
\end{verbatim}

После завершения работы корпус доступен в директории \texttt{data/corpus.ndjson}.

\pagebreak
