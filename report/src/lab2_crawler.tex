\section{Поисковый робот}

\subsection{Архитектура робота}

Поисковый робот реализован на языке Python с использованием асинхронной загрузки контента. Основные библиотеки: \texttt{aiohttp} (асинхронные HTTP-запросы), \texttt{BeautifulSoup} (парсинг HTML).

Скрапер поддерживает два режима работы:
\begin{itemize}
    \item \textbf{Wikipedia} (\texttt{python scraper.py wiki}) --- обход категорий Википедии, сбор $\sim$30\,000 статей в \texttt{corpus.ndjson}
    \item \textbf{КиберЛенинка} (\texttt{python scraper.py cyberleninka}) --- последовательный обход каталога научных статей, сбор $\sim$500 статей в \texttt{corpus2.ndjson}
\end{itemize}

Оба корпуса загружаются в единый индекс поискового движка.

\subsection{Алгоритм работы}

\textbf{Фаза 1 --- обнаружение статей:}
\begin{enumerate}
    \item Загрузка списка seed-категорий из конфигурационного файла \texttt{config.yaml}.
    \item Для каждой категории --- запрос к Wikipedia API (\texttt{action=query\&list=categorymembers}) для получения статей и подкатегорий.
    \item Рекурсивный обход подкатегорий (до глубины 10) с параллельным выполнением (батчи по 10).
    \item Дедупликация по заголовку статьи.
\end{enumerate}

\textbf{Фаза 2 --- загрузка контента:}
\begin{enumerate}
    \item Для каждой обнаруженной статьи --- запрос к API (\texttt{action=parse}) для получения HTML.
    \item Извлечение текста из HTML: параграфы, списки, заголовки.
    \item Фильтрация по минимальной длине текста.
    \item Дедупликация по каноническому заголовку (разрешение редиректов).
    \item Сохранение в NDJSON.
    \item Параллельная обработка (до 30 одновременных запросов).
\end{enumerate}

\subsection{Критерии фильтрации}

Для обеспечения качества корпуса применяются следующие фильтры:

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Критерий} & \textbf{Значение} \\
\midrule
Минимальная длина текста & 1000 слов \\
Исключение служебных страниц & Категории, шаблоны, обсуждения, файлы \\
Исключение дубликатов & По нормализованному URL \\
\bottomrule
\end{tabular}
\caption{Критерии фильтрации документов}
\end{table}

Список исключаемых паттернов URL:
\begin{itemize}
    \item \texttt{Special:}, \texttt{Служебная:} --- служебные страницы
    \item \texttt{File:}, \texttt{Файл:} --- страницы файлов
    \item \texttt{Category:}, \texttt{Категория:} --- страницы категорий (используются только для навигации)
    \item \texttt{Template:}, \texttt{Шаблон:} --- шаблоны
    \item \texttt{Talk:}, \texttt{Обсуждение:} --- страницы обсуждений
    \item \texttt{User:}, \texttt{Участник:} --- страницы участников
    \item \texttt{Wikipedia:}, \texttt{Википедия:} --- внутренние страницы Википедии
\end{itemize}

\subsection{Формат выходных данных}

Каждая строка файла \texttt{corpus.ndjson} содержит один JSON-объект:

\begin{verbatim}
{"url": "https://ru.wikipedia.org/wiki/Алфавит",
 "title": "Алфавит",
 "text": "Алфавит — совокупность букв или символов...",
 "word_count": 4521,
 "paragraph_count": 38}
\end{verbatim}

\subsection{Обработка проблемных ситуаций}

\begin{itemize}
    \item \textbf{Таймауты}: при отсутствии ответа в течение 30 секунд запрос пропускается.
    \item \textbf{Ошибки HTTP}: страницы с кодами 4xx/5xx пропускаются.
    \item \textbf{Кодировка}: все тексты приводятся к UTF-8.
    \item \textbf{Ограничение нагрузки}: задержка между запросами предотвращает блокировку.
    \item \textbf{User-Agent}: используется идентификатор \texttt{LinguisticsSearchBot/1.0}.
\end{itemize}

\subsection{Скрапер КиберЛенинки}

Для КиберЛенинки реализован отдельный скрапер (\texttt{CyberLeninkaScraper}), работающий по упрощённой схеме:
\begin{enumerate}
    \item Последовательный обход страниц каталога <<Языкознание и литературоведение>> (\texttt{/article/c/languages-and-literature/\{page\}}) --- каждая страница содержит $\sim$20 ссылок на статьи
    \item Для каждой ссылки --- загрузка полной страницы статьи
    \item Извлечение текста из раздела <<Текст научной работы>> (после заголовка h2), с остановкой на <<Список литературы>>
    \item Фильтрация по минимальной длине (1\,000 слов)
    \item Сохранение в \texttt{corpus2.ndjson}
\end{enumerate}

В отличие от Википедии, не требуется обход ссылок внутри статей --- статьи берутся непосредственно из каталога. Задержка между запросами увеличена до 300~мс для снижения нагрузки на сервер.

\subsection{Контейнеризация}

Скраперы упакованы в Docker-контейнеры и запускаются командами:
\begin{verbatim}
docker compose run --rm scraper
docker compose run --rm scraper-cyberleninka
\end{verbatim}

После завершения работы корпуса доступны в файлах \texttt{data/corpus.ndjson} и \texttt{data/corpus2.ndjson}.

\subsection{Журнал выполнения задания}

При разработке и запуске поискового робота были выявлены следующие проблемы и их решения:

\begin{enumerate}
    \item \textbf{Ограничение скорости запросов к Wikipedia API}: Wikipedia API имеет лимиты на количество запросов в секунду. При параллельной обработке более 30 одновременных запросов возникали ошибки 429 (Too Many Requests). Решение: ограничение параллелизма до 30 запросов и добавление задержек между батчами запросов (0.1 секунды).
    
    \item \textbf{Дублирование статей через разрешение редиректов}: Многие статьи в Википедии имеют редиректы (например, <<Толстой>> редиректит на <<Лев Толстой>>). При обходе категорий обе версии попадали в список, что приводило к дублированию. Решение: использование канонического заголовка статьи через параметр \texttt{redirects=1} в API и дедупликация по нормализованному заголовку перед сохранением.
    
    \item \textbf{Управление памятью при больших объёмах данных}: При обходе категорий накапливался список из ~50\,000 названий статей в памяти. Для загрузки контента использовалась потоковая обработка: каждая статья обрабатывалась и сразу записывалась в файл, без накопления всего корпуса в памяти. Это позволило обрабатывать корпус на машинах с ограниченной оперативной памятью.
    
    \item \textbf{Обработка таймаутов}: Некоторые статьи Википедии очень большие (десятки тысяч символов), и их загрузка через API могла превышать 30 секунд. Решение: установка таймаута на уровне HTTP-клиента (aiohttp) и пропуск проблемных статей с логированием ошибок для последующего анализа.
    
    \item \textbf{Асинхронная архитектура}: Изначально использовался синхронный подход с библиотекой \texttt{requests}, что приводило к длительному времени выполнения (за 10 часов для не удалось собрать 30\,500 статей). Переход на асинхронный подход с \texttt{aiohttp} сократил время до ~2--3 часов при том же объёме данных.
\end{enumerate}

\subsection{Выводы}

Поисковый робот успешно реализован с использованием асинхронного подхода и обеспечивает сбор корпуса из ~30\,000 документов за приемлемое время.

\textbf{Анализ асинхронного подхода:}
Асинхронная архитектура с использованием \texttt{aiohttp} показала значительное преимущество по сравнению с синхронной реализацией:
\begin{itemize}
    \item Ускорение обработки раза за счёт параллельных HTTP-запросов
    \item Эффективное использование сетевых ресурсов: пока один запрос ожидает ответа, обрабатываются другие
    \item Масштабируемость: легко увеличить параллелизм при наличии достаточной пропускной способности сети
\end{itemize}


\textbf{Недостатки и ограничения:}
\begin{itemize}
    \item Зависимость от доступности Wikipedia API и его лимитов
    \item Отсутствие механизма возобновления после полного сбоя (необходимо перезапускать обход категорий)
    \item Фильтрация по длине текста может исключать короткие, но релевантные статьи (например, статьи о малоизвестных произведениях)
    \item HTML-парсинг может пропускать текст, встроенный в сложные шаблоны Википедии
\end{itemize}

\textbf{Возможные улучшения:}
\begin{itemize}
    \item Реализация механизма чекпоинтов для возобновления обхода с последней обработанной категории
    \item Использование дампа Википедии вместо API для более быстрого и полного сбора данных
\end{itemize}

\pagebreak
